{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(27,94,32); background: rgb(200,230,201); border: solid 1px rgb(129,199,132); padding: 10px;\">\n",
    "In this lab, you'll get a chance to experiment with library implementations of the dimensionality reduction techniques discussed in this week's lectures.\n",
    "\n",
    "We will be performing dimensionality reduction on gene-expression data from different tissues of fruit flies.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation notes\n",
    "\n",
    "To run this notebook you will need to install several packages. These can be installed via pip.\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install pandas scikit-learn umap-learn matplotlib seaborn ipykernel\n",
    "```\n",
    "\n",
    "> **Note** <br>\n",
    "> Ensure pip is updated to 23.2.1 or later, otherwise you may experience errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap  # You can ignore the warnings about Numba\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Display 2 decimal places for floats\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# Set default figure size to a larger size\n",
    "plt.rcParams['figure.figsize'] = [8, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using microarray gene-expression data from [FlyAtlas](http://flyatlas.org/atlas.cgi), part of [NCBI's Gene Expression Omnibus](https://www.ncbi.nlm.nih.gov/geo/). \n",
    "\n",
    "The dataset includes gene expression information from a variety of cell types. The file is present relative to this notebook at <small>`./data/flydata.txt.gz`</small>. \n",
    "\n",
    "Columns\n",
    "- Each column represents a sample. \n",
    "- These 136 columns represent 4 replicates each from 34 different tissue types / developmental stages.\n",
    "\n",
    "Rows\n",
    "- Each row represents a microarray probe. \n",
    "- These 18952 probes can be mapped to gene names but we will skip that for today. \n",
    "\n",
    "Run the following cell to load the data. \n",
    "\n",
    "> Note \n",
    "> - <small>`gzip`</small> is used to open the file as it is gzip compressed. \n",
    "> - <small>`comment='!'`</small> tells Pandas to treat any lines starting with '!' as comments. <br>\n",
    "  This is useful because the flydata.txt file has header lines providing documentation.\n",
    "> - <small>`index_col=0`</small> tells Pandas to use the first column as index. \n",
    "> - <small>`sep='\\t'`</small> tells Pandas that the flydata.txt file is tab-separated. \n",
    "> - The original sample names (columns) are uninformative, so we pull their labels from the header comments and reassign. \n",
    "> \n",
    "> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read expression data from file\n",
    "with gzip.open('./data/flydata.txt.gz', 'rt') as fp:\n",
    "    expression = pd.read_csv(fp, sep='\\t', comment='!', index_col=0)\n",
    "    \n",
    "# search the header lines for sample descriptions so columns can be relabelled. \n",
    "with gzip.open('./data/flydata.txt.gz', 'rt') as fp:\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        if line.startswith(\"!Sample_title\"):\n",
    "            header = [x.strip('\"') for x in line.split(\"\\t\")[1:]]\n",
    "            expression.columns = header\n",
    "            break\n",
    "        line = fp.readline()\n",
    "\n",
    "expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cell Type Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet uses regular expressions and custom functions to generate metadata using sample names. <br>\n",
    "These will be used as categories for plotting and labels later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_development(text: str) -> str:\n",
    "    if 'Adult' in text:\n",
    "        return 'Adult'\n",
    "    elif 'Larval Wandering' in text:\n",
    "        return 'Larval Wandering'\n",
    "    elif 'Larval Feeding' in text:\n",
    "        return 'Larval Feeding'\n",
    "    elif 'S2 Cells' in text:\n",
    "        return 'Passaged Cells'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def get_tissue(row: pd.Series) -> str:\n",
    "    sample_words = row['sample'].split(' ')\n",
    "    develop_words = row['development'].split(' ')\n",
    "    return ' '.join([x for x in sample_words if x not in develop_words])\n",
    "\n",
    "PATTERN = r'(.+?)(( biological)? rep\\d+)'\n",
    "meta = pd.DataFrame()\n",
    "meta['ID'] = expression.columns.to_list()\n",
    "meta['sample'] = meta['ID'].apply(lambda x: re.match(PATTERN, x).group(1))\n",
    "meta['sample'] = meta['sample'].apply(lambda x: x.replace('Larvae', 'Larval'))\n",
    "meta['development'] = meta['sample'].apply(get_development)\n",
    "meta['tissue'] = meta.apply(get_tissue, axis=1)\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Expression Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting the expression data, it's common practice to take the log of expression values. <br>\n",
    "**Log transforming** count data is very common practise in data science, especially biology. <br>\n",
    "Let's see why:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log transformed expression values\n",
    "log_expression = np.log(expression + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original transcript counts\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "ax_raw = axes[0]\n",
    "ax_log = axes[1]\n",
    "sns.histplot(expression.values.flatten(), bins=50, ax=ax_raw, stat='proportion')\n",
    "sns.histplot(log_expression.values.flatten(), bins=50, ax=ax_log, stat='proportion')\n",
    "ax_raw.set_title('raw')\n",
    "ax_raw.set_xlabel('counts')\n",
    "ax_log.set_title('log2')\n",
    "ax_log.set_xlabel('counts')\n",
    "ax_log.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a small number of very highly expressed genes in our dataset.  Variation in these genes may have a disproportionately large effect on attempts to find structure in our data (highly expressed genes will have higher varances). By log transforming our data we can map expression values to an approximately normal distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Expression Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will render a two-dimensional scatterplot which is coloured by the list of categories. <br>\n",
    "We will use it for PCA, MDS, tSNE, UMAP, and LDA visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_two_dimensions(data: pd.DataFrame, meta: pd.DataFrame, label: str='tissue'):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    # Define colors and markers to cycle through\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "            '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    markers = ['o', 's', '^', 'D', '*', 'h', '>']\n",
    "    combinations = list(itertools.product(markers, colors))\n",
    "\n",
    "    # developmental stage\n",
    "    i = 0\n",
    "    for group, samples in meta.groupby(label)['ID'].agg(list).to_dict().items():\n",
    "        df = data.loc[samples].copy()\n",
    "        ax.scatter(\n",
    "            df[0], df[1], label=group, c=combinations[i][1], marker=combinations[i][0],\n",
    "            s=50, alpha=0.7, edgecolors='black', linewidths=0.5\n",
    "        )\n",
    "        i += 1\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=True)\n",
    "    ax.set_title(label)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs PCA on the dataset.  \n",
    "\n",
    "Here is an intro video on PCA [StatQuest: Principal Component Analysis (PCA)](https://www.youtube.com/watch?v=FgakZw6K1QQ) if you want a refresher. \n",
    "\n",
    "Steps:\n",
    "1. Initialise the PCA reducer class \n",
    "2. Transpose the dataframe so each row is a sample (currently they're columns)\n",
    "3. Perform pca using `.fit()` to identify low-dimension mapping \n",
    "4. Transform the original data into low-dimensional space using `.transform()`\n",
    "5. Plot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. initialise reducer\n",
    "pca = PCA()\n",
    "\n",
    "# 2. transpose matrix so each row is a sample \n",
    "input_df = log_expression.T\n",
    "print(f\"high-dimensional data: {input_df.shape[0]} samples, {input_df.shape[1]} probes.\\n\")\n",
    "print(input_df.iloc[:5, :5])\n",
    "\n",
    "# 3. perform pca to identify low-dimension mapping\n",
    "pca.fit(input_df)\n",
    "\n",
    "# 4. transform the original data into low-dimensional space using learned mapping\n",
    "result = pca.transform(input_df)\n",
    "\n",
    "# 5. plot \n",
    "# turn the numpy nd.array into a dataframe with sample ID for plotting\n",
    "result_df = pd.DataFrame(result, index=input_df.index.to_list())\n",
    "print(f\"\\nlow-dimensional data: {result_df.shape[0]} samples, {result_df.shape[1]} dimensions.\\n\")\n",
    "print(result_df.iloc[:5, :5])\n",
    "\n",
    "# visualse first two dimensions \n",
    "plot_two_dimensions(result_df, meta, 'tissue')\n",
    "plot_two_dimensions(result_df, meta, 'development')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these visualisations, there are a number of interesting observations we can make\n",
    "- *'Malphingian tubule'* tissue samples have similar expression to general *'Tubule'* tissue (bottom middle).\n",
    "- *'Brain', 'Head', 'Eye', 'Central Nervous System', and 'Thoracoabdominal Gangleon'* tissue cluster together (top right). \n",
    "- Developmental stages display separation, with some exceptions (which make biological sense).\n",
    "    - *'Mid Gut'* tissue is similar in both Adults & Larval Feeding (Bottom middle)\n",
    "    - *'Central Nervous System'* tissue in Larval Feeding is similar to Adult *'Brain', 'Head', 'Eye'* and *'Thoracoabdominal Gangleon'* tissue (top right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Transforming\n",
    "\n",
    "**`.fit()` then `.transform()`**\n",
    "\n",
    "In the example above we learned the mapping and transformed the data in two steps.\n",
    "\n",
    "This can be handy for ***supervised machine learning*** tasks where the data is split into a training and test set.  In situations where your data is high-dimensional (ie #features > #observations), you should perform feature selection or feature extraction to improve model performance. If using feature selection there is no issue, as the selected features are present in both the training and test set. On the other hand, feature extraction *learns* a mapping based on input data! In these situations you have to <small>`.fit()`</small> PCA on the training dataset only, then <small>`.transform()`</small> both the training and test sets into low-dimensional space using the same learned mapping. PCA and LDA are good choices for feature extraction as can reduce thousands of dimensions into a user-defined number (eg, 2, 5, 10 etc). \n",
    "\n",
    "\n",
    "**`.fit_transform()`**\n",
    "\n",
    "If your task is unsupervised (ie for visualisation / clustering), you can use <small>`.fit_transform(matrix)`</small> instead which performs both steps together. \n",
    "\n",
    "Here is an example using PCA. \n",
    "\n",
    "<div style='font-size: 18px'>\n",
    "\n",
    "```bash\n",
    "reducer = PCA(n_components=2)  # PCA | MDS | TSNE | UMAP \n",
    "result = reducer.fit_transform(data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "In the remaining exercises we will perform dimensionality reducion using MDS, tSNE, and UMAP. \n",
    "\n",
    "Similar to PCA, the packages we will use all follow the <small>`.fit_transform(matrix)`</small> syntax. <br>\n",
    "Each reduction technique has a different set of parameters aside from <small>`n_components`</small>. <br>\n",
    "For example, you can supply a value for the <small>`perplexity`</small> parameter to TSNE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: PCA Explained Variance\n",
    "\n",
    "This code makes a plot of the explained variance by component, like we saw on one of the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_[:5])\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot([x + 1 for x in range(len(pca.explained_variance_ratio_))], pca.explained_variance_ratio_, 'o-')\n",
    "plt.xlabel(\"principal component\")\n",
    "plt.ylabel(\"variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that the ammount of additional explained variance diminishes as we all more components.\n",
    "\n",
    "How many components do you think are worth keeping for our analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: rgb(27,94,32); background: rgb(200,230,201); border: solid 1px rgb(129,199,132); padding: 10px;\">\n",
    "<b>Challenge:</b> Calculate the number of components required to explain at least 90% of the variance in our data.\n",
    "\n",
    "Generally, we take as many components as necessary to cover 90% of the sample variance. \n",
    "\n",
    "We will just take the cumulative sum until we have over 90% explained variance, and report how many components we needed.\n",
    "    \n",
    "Hint: You can find the variance explained by each component with `pca.explained_variance_ratio_`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "E1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Multidimensional Scaling (MDS)\n",
    "\n",
    "<div style=\"color: rgb(27,94,32); background: rgb(200,230,201); border: solid 1px rgb(129,199,132); padding: 10px;\">\n",
    "<b>Challenge:</b> Try creating an MDS plot using `MDS()`, which we imported from `sklearn.manifold`. All scikit-learn models use a consistent syntax, so the syntax is extremely similar to that for `PCA()`.\n",
    "\n",
    "This should produce a near-identical plot to PCA as we are not changing the MDS distance metric from euclidean (default). \n",
    "\n",
    "Examine the documentation either online or just using `help(MDS)` in the notebook.\n",
    "\n",
    "</div>\n",
    "\n",
    "StatQuest intro to [MDS and PCoA](https://youtu.be/GEn-_dAyYME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "E2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: tSNE\n",
    "\n",
    "<div style=\"color: rgb(27,94,32); background: rgb(200,230,201); border: solid 1px rgb(129,199,132); padding: 10px;\">\n",
    "<b>Challenge:</b> Try creating a tSNE plot using `TSNE()`, which we imported from `sklearn.manifold`. All scikit-learn models use a consistent syntax, so the syntax is extremely similar to that for `PCA()`.\n",
    "\n",
    "Examine the documentation either online or just using `help(TSNE)` in the notebook.\n",
    "\n",
    "`TSNE()` takes several parameters: the most important is `perplexity`. Lower values of perplexity try hard to preserve local structure at the cost of global structure, and vice versa. From the documentation, what is the default value of `perplexity`? What happens if you redo your plot with it set to a much lower or much higher value?\n",
    "</div>\n",
    "\n",
    "Helpful resources for learning tSNE:\n",
    "\n",
    "[StatQuest: t-SNE, Clearly Explained](https://www.youtube.com/watch?v=NEaUSP4YerM)\n",
    "\n",
    "[t-SNE interactive settings](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "[Datacamp: t-SNE Tutorial](https://www.datacamp.com/community/tutorials/introduction-t-sne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "E3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: UMAP\n",
    "\n",
    "<div style=\"color: rgb(27,94,32); background: rgb(200,230,201); border: solid 1px rgb(129,199,132); padding: 10px;\">\n",
    "<b>Challenge:</b> Try creating a UMAP plot using `UMAP()`, which we imported from the `umap` library. `umap` is not part of scikit-learn, but it deliberately uses a similar syntax.\n",
    "\n",
    "Examine the documentation either online or just using `help(UMAP)` in the notebook.\n",
    "\n",
    "Look at the available parameters in the documentation. Try varying `n_neighbours` (which has a conceptual similarity to tSNE's `perplexity`) and `min_dist`.\n",
    "</div>\n",
    "\n",
    "[UMAP Uniform Manifold Approximation and Projection for Dimension Reduction](https://www.youtube.com/watch?v=nq6iPZVUxZU) video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "E4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Linear Discriminant Analysis (LDA)\n",
    "\n",
    "While PCA and LDA are both linear dimensionality reduction techniques, they have a key difference. \n",
    "- PCA aims to maximise total data variance in low-dimensional space. \n",
    "- LDA aims to maximise class separation in low-dimensional space. \n",
    "\n",
    "In this manner, LDA is a supervised approach and therefore requires class labels. \n",
    "\n",
    "<img src='./media/lda_pca.png' width=500>\n",
    "\n",
    "\n",
    "<div style=\"color: rgb(27,94,32); background: rgb(200,230,201); border: solid 1px rgb(129,199,132); padding: 10px;\">\n",
    "<b>Challenge:</b> Try creating LDA plots using `LinearDiscriminantAnalysis()`, which we imported from `sklearn.discriminant_analysis`.\n",
    "\n",
    "Perform LDA twice, first using <small>`'development'`</small> as sample labels, then <small>`'tissue'`</small> as sample labels. \n",
    "\n",
    "You will need to use the <small>`meta`</small> dataframe to associate samples to labels. \n",
    "\n",
    "For help, examine the documentation [online](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) or using `help(LinearDiscriminantAnalysis)` in the notebook.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
